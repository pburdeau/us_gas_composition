{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900037b-4e9a-43e0-bb84-42d08383c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy.linalg as linalg\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import datetime\n",
    "from datetime import timedelta, datetime, timezone\n",
    "import tqdm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import LightSource\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import skgstat as skg\n",
    "from skgstat import models\n",
    "import sklearn as sklearn\n",
    "from sklearn.neighbors import KDTree\n",
    "import math\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.interpolate import Rbf\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.tri as tri\n",
    "#import argparse\n",
    "import shapely\n",
    "import glob\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import pickle\n",
    "\n",
    "# define paths\n",
    "root_path = '/scratch/users/pburdeau/data/gas_composition' # root_path to project\n",
    "ghgrp_path = os.sep.join([root_path, 'ghgrp', 'ghgrp_v2.csv']) # path to ghgrp data\n",
    "ghgrp = pd.read_csv(ghgrp_path)\n",
    "shapefiles_path = root_path + '/shapefiles'\n",
    "usgs_path = root_path + '/usgs/usgs.csv'\n",
    "out_path = root_path + '/out'\n",
    "\n",
    "# Clean USGS\n",
    "\n",
    "# Download geographic data: shale plays, counties, states, basins\n",
    "us_states = gpd.read_file(shapefiles_path + '/cb_2018_us_state_20m/cb_2018_us_state_20m.shp')\n",
    "\n",
    "# Clean us_counties to keep only the ones in usgs\n",
    "us_counties = gpd.read_file(shapefiles_path + '/cb_2018_us_county_500k/cb_2018_us_county_500k.shp')\n",
    "basins_gdf = gpd.read_file(shapefiles_path + '/basins_shapefiles')\n",
    "\n",
    "\n",
    "index_to_deletes = []\n",
    "for i in range(len(us_counties)):\n",
    "    county_name = us_counties['NAME'].iloc[i].upper()\n",
    "    state_fp = us_counties['STATEFP'].iloc[i]\n",
    "    if len(us_states[us_states.STATEFP == state_fp]) > 0:\n",
    "        state_name = us_states[us_states.STATEFP == state_fp].reset_index(drop=True).NAME.iloc[0].upper()\n",
    "        combined_name = f\"{county_name} ({state_name})\"\n",
    "        us_counties['NAME'].iloc[i] = combined_name\n",
    "        us_counties['COUNTY_NAME'] = county_name\n",
    "        us_counties['STATE_NAME'] = state_name\n",
    "    else:\n",
    "        index_to_deletes.append(i)\n",
    "        \n",
    "for i in index_to_deletes:\n",
    "    us_counties = us_counties.drop(i)\n",
    "\n",
    "\n",
    "usgs = pd.read_csv(usgs_path)\n",
    "usgs_not_na = usgs[~pd.isna(usgs.LAT)]\n",
    "usgs_not_na = usgs_not_na[~pd.isna(usgs.LONG)]\n",
    "usgs_not_na = usgs_not_na[~pd.isna(usgs.FINAL_SAMPLING_DATE)]\n",
    "\n",
    "years = []\n",
    "for i in range(len(usgs_not_na)):\n",
    "    years.append(int(usgs_not_na.FINAL_SAMPLING_DATE.iloc[i][-4:]))\n",
    "usgs_not_na['Year'] = years\n",
    "\n",
    "usgs_gdf = gpd.GeoDataFrame(usgs_not_na, geometry=gpd.points_from_xy(usgs_not_na.LONG, usgs_not_na.LAT), crs=\"EPSG:4326\")\n",
    "\n",
    "usgs_gdf = usgs_gdf.to_crs(26914)\n",
    "\n",
    "usgs_gdf = usgs_gdf.rename(columns={'API':'API14'})\n",
    "\n",
    "usgs_gdf['COUNTY'] = [str(usgs_gdf.COUNTY.iloc[i]) + ' (' + usgs_gdf.STATE.iloc[i] + ')' for i in range(len(usgs_gdf))]\n",
    "\n",
    "# transform dates in datetime format\n",
    "dates = []\n",
    "with tqdm(total=len(usgs_gdf)) as pbar:\n",
    "    for i in range(len(usgs_gdf)):\n",
    "        month, day, year = usgs_gdf['FINAL_SAMPLING_DATE'].iloc[i].split('/')\n",
    "        month = int(month)\n",
    "        day = int(day)\n",
    "        year = int(year)\n",
    "        if month == 0:\n",
    "            month = 1\n",
    "        if day == 0:\n",
    "            day = 1\n",
    "        if (month == 2) and (day > 28):\n",
    "            day = 28\n",
    "        if (day > 30):\n",
    "            day = 30\n",
    "        if month > 12:\n",
    "            month = 12\n",
    "        dates.append(datetime(year, month, day, 0, 0, 0))\n",
    "        pbar.update(1)\n",
    "    usgs_gdf['date'] = dates    \n",
    "\n",
    "# Transform datetime in number of days since earliest date\n",
    "usgs_gdf = usgs_gdf.sort_values(by='date').reset_index(drop=True)\n",
    "epochs = [0]\n",
    "for i in range(1, len(usgs_gdf)):\n",
    "    epochs.append((usgs_gdf.date.iloc[i] - usgs_gdf.date.iloc[0]).days)\n",
    "usgs_gdf['epochs'] = epochs\n",
    "\n",
    "usgs_gdf = usgs_gdf.drop(columns=['ID', 'SOURCE','BLM ID','USGS ID','ALASKA_QUAD_NAMES','WELL NAME','PUBLICATION DATE','COUNTY','STATE'])\n",
    "\n",
    "usgs_gdf = gpd.sjoin(usgs_gdf.to_crs(26914), basins_gdf.to_crs(26914), op='within')\n",
    "\n",
    "usgs_gdf = usgs_gdf.to_crs(26914)\n",
    "\n",
    "usgs_gdf['X'] = usgs_gdf.geometry.x\n",
    "usgs_gdf['Y'] = usgs_gdf.geometry.y\n",
    "usgs_gdf['T'] = np.array(usgs_gdf.epochs)\n",
    "\n",
    "usgs_gdf = usgs_gdf.drop(columns=['FINAL_DRILLING_DATE', 'FINAL_SAMPLING_DATE', 'SAMPLE DATE BEFORE COMPLETION', 'index_right', 'BASIN_CODE', 'DEPTH','AGE','FORMATION','geometry','FIELD','LAT','LONG'])\n",
    "\n",
    "numeric_columns = ['HE', 'CO2', 'H2', 'N2', 'H2S', 'AR', 'O2', 'C1', 'C2', 'C3', 'N-C4', 'I-C4', 'N-C5', 'I-C5', 'C6+']\n",
    "\n",
    "usgs_gdf[numeric_columns] = usgs_gdf[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "# Ensure 'geometry' column exists\n",
    "usgs_gdf['geometry'] = gpd.points_from_xy(usgs_gdf['X'], usgs_gdf['Y'])\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "usgs_gdf = gpd.GeoDataFrame(usgs_gdf, geometry='geometry', crs=\"EPSG:26914\")\n",
    "\n",
    "# Keep API that are NaNs\n",
    "key_columns = ['X', 'Y', 'Year', 'date', 'T', 'BASIN_NAME']\n",
    "aggregated_df = usgs_gdf.groupby(key_columns).agg({col: np.nanmean for col in numeric_columns}).reset_index()\n",
    "aggregated_df.to_csv(root_path + '/usgs/usgs_processed_with_nanapis.csv')\n",
    "\n",
    "# Upload production data\n",
    "\n",
    "wells_data = []\n",
    "basin_list = usgs_gdf.BASIN_NAME.unique()\n",
    "with tqdm(total=len(basin_list)) as pbar:\n",
    "    for basin_name in basin_list:\n",
    "        basin_file_path = os.path.join(root_path, f'wells_info_prod_per_basin/{basin_name}_final.csv')\n",
    "        if os.path.isfile(basin_file_path):\n",
    "            basin_df = pd.read_csv(basin_file_path)\n",
    "            wells_data.append(basin_df)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Merge usgs data that has API with production data\n",
    "\n",
    "usgs_prod = pd.merge(wells_data[['API14', 'Year', 'Monthly Gas', 'Monthly Oil', 'BASIN_NAME', 'X', 'Y', 'T']], usgs_gdf[['API14', 'HE', 'CO2', 'H2', 'N2', 'H2S', 'AR', 'O2', 'C1', 'C2', 'C3',\n",
    "       'N-C4', 'I-C4', 'N-C5', 'I-C5', 'C6+', 'Year', 'X', 'Y', 'T']], on=['Year', 'API14'])\n",
    "usgs_prod = usgs_prod.rename(columns={'X_x':'X_prod', 'Y_x':'Y_prod', 'T_x':'T_prod',\n",
    "                                     'X_y':'X', 'Y_y':'Y', 'T_y':'T'})\n",
    "group_cols = ['API14', 'Year', 'X_prod', 'Y_prod', 'T_prod', 'BASIN_NAME']\n",
    "mean_cols = ['Monthly Gas', 'Monthly Oil', 'HE', 'CO2', 'H2', 'N2', 'H2S',\n",
    "             'AR', 'O2', 'C1', 'C2', 'C3', 'N-C4', 'I-C4', 'N-C5', 'I-C5', 'C6+']\n",
    "\n",
    "df_grouped = usgs_prod.groupby(group_cols, dropna=False)[mean_cols].mean().reset_index()\n",
    "df_grouped.to_csv(root_path + '/usgs/usgs_prod.csv', index=False)\n",
    "\n",
    "# Correlation analysis\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load dataset\n",
    "df = usgs_prod.copy()\n",
    "\n",
    "# Compute Gas-to-Oil Ratio (GOR) while handling NaN values\n",
    "df[\"GOR\"] = df[\"Monthly Gas\"] / df[\"Monthly Oil\"]\n",
    "df[\"Log GOR\"] = np.log(df[\"GOR\"])\n",
    "\n",
    "# Handle NaN and infinite values\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(subset=[\"Log GOR\"], inplace=True)\n",
    "lower, upper = np.percentile(df[\"Log GOR\"].dropna(), [0, 100])\n",
    "df_filtered = df[(df[\"Log GOR\"] >= lower) & (df[\"Log GOR\"] <= upper)].copy()\n",
    "\n",
    "# Define all components\n",
    "components = ['C1', 'C2', 'C3', 'N-C4', 'I-C4', 'N-C5', 'I-C5', 'C6+']\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"crest\", n_colors=len(components))[::-1]  # Shades of blue\n",
    "\n",
    "# Set up subplots without shared axes\n",
    "n_rows, n_cols = 2, 4  # Adjust grid to fit all components\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 6))\n",
    "axes = axes.flatten()  # Flatten axes for easy iteration\n",
    "\n",
    "for i, (comp, color) in enumerate(zip(components, palette)):\n",
    "    ax = axes[i]\n",
    "    valid_mask = df_filtered[\"Log GOR\"].notna() & df_filtered[comp].notna()\n",
    "    \n",
    "    if valid_mask.sum() > 1:\n",
    "        ax.scatter(df_filtered.loc[valid_mask, \"Log GOR\"], \n",
    "                   df_filtered.loc[valid_mask, comp], \n",
    "                   alpha=0.3, color=color, edgecolors=\"none\")\n",
    "\n",
    "        slope, intercept, r_value, _, _ = linregress(df_filtered.loc[valid_mask, \"Log GOR\"], df_filtered.loc[valid_mask, comp])\n",
    "        x_vals = np.linspace(df_filtered[\"Log GOR\"].min(), df_filtered[\"Log GOR\"].max(), 100)\n",
    "        ax.plot(x_vals, slope * x_vals + intercept, color=\"black\")\n",
    "        ax.text(0.05, 0.9, f\"r={r_value:.3f}\", transform=ax.transAxes, bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "    \n",
    "    ax.set_title(comp, fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Log GOR\")\n",
    "    ax.set_ylabel(f\"% {comp}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures_out/correlation_plots.png', dpi=300)\n",
    "# Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "components = ['HE', 'CO2', 'H2', 'N2', 'H2S', 'AR', 'O2','C1', 'C2', 'C3', 'N-C4', 'I-C4', 'N-C5', 'I-C5', 'C6+']\n",
    "for comp in components:\n",
    "    valid_mask = df_filtered[\"Log GOR\"].notna() & df_filtered[comp].notna()\n",
    "    slope, intercept, r_value, _, _ = linregress(df_filtered.loc[valid_mask, \"Log GOR\"], df_filtered.loc[valid_mask, comp])\n",
    "    print(f'Comp: {comp}')\n",
    "    print(f\"r={r_value:.3f}\")\n",
    "\n",
    "# Add 'T' info to production files\n",
    "\n",
    "input_folder = root_path + '/wells_prod_jan_26' # path for production data without 'T' column\n",
    "output_folder = root_path + '/wells_info_prod_per_basin'\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List existing files in the output folder\n",
    "existing_files = set(os.listdir(output_folder))\n",
    "\n",
    "# Iterate over all CSV files in the input folder\n",
    "with tqdm(total=len(os.listdir(input_folder))) as pbar:\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith('.csv') and 'Anadarko' in file:\n",
    "            print(file)\n",
    "            # Check if the file already exists in the output folder\n",
    "#             if file in existing_files:\n",
    "#                 print(f\"File '{file}' already processed. Skipping...\")\n",
    "#                 pbar.update(1)\n",
    "#                 continue  # Skip this file\n",
    "\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Add geometry and convert to UTM Zone 14N\n",
    "            df['geometry'] = df.apply(\n",
    "                lambda row: Point(row['Surface Hole Longitude (WGS84)'], row['Surface Hole Latitude (WGS84)']),\n",
    "                axis=1\n",
    "            )\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "            del df\n",
    "            gdf = gdf.to_crs(26914)  # Convert to UTM Zone 14N\n",
    "            gdf['X'] = gdf.geometry.x\n",
    "            gdf['Y'] = gdf.geometry.y\n",
    "            gdf = gdf.drop(columns='geometry')\n",
    "\n",
    "            # Convert 'Monthly Production Date' to datetime and extract the year\n",
    "            gdf['Monthly Production Date'] = pd.to_datetime(gdf['Monthly Production Date'])\n",
    "            gdf['Year'] = gdf['Monthly Production Date'].dt.year\n",
    "\n",
    "            # Group by API14, X, Y, and Year, aggregating Monthly Gas and Monthly Oil\n",
    "            grouped = gdf.groupby(['API14', 'X', 'Y', 'Year'], as_index=False).agg({\n",
    "                'Monthly Gas': 'sum',\n",
    "                'Monthly Oil': 'sum',\n",
    "                'BASIN_NAME': 'first'  # Retain the first value of BASIN_NAME\n",
    "            })\n",
    "\n",
    "            # Compute the middle of the year for each group\n",
    "            grouped['Last Date'] = grouped['Year'].apply(lambda year: pd.Timestamp(f'{year}-12-31'))  # End of the year\n",
    "\n",
    "            # Calculate T based on the middle date\n",
    "            grouped['T'] = (grouped['Last Date'] - pd.Timestamp(reference_date)).dt.days\n",
    "\n",
    "            # Drop the temporary 'Middle Date' column\n",
    "            grouped = grouped.drop(columns=['Last Date'])\n",
    "\n",
    "            # Save the grouped data to the output directory\n",
    "            output_file_path = os.path.join(output_folder, file)\n",
    "            grouped.to_csv(output_file_path, index=False)\n",
    "            print(f\"Saved grouped data to: {output_file_path}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "# Clean ghgrp data\n",
    "\n",
    "ghgrp_indsegm_not_na = ghgrp[ghgrp.industry_segment.notna()]\n",
    "ghgrp_production = ghgrp_indsegm_not_na[ghgrp_indsegm_not_na.industry_segment.str.contains('Onshore petroleum and natural gas production')]\n",
    "ghgrp_production = ghgrp_production[ghgrp_production.ch4_average_mole_fraction > 0].reset_index(drop=True)\n",
    "ghgrp_production = ghgrp_production[ghgrp_production.ch4_average_mole_fraction > ghgrp_production.co2_average_mole_fraction].reset_index(drop=True)\n",
    "\n",
    "state_abbreviations_to_names = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NY': 'New York',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "\n",
    "# Sample data\n",
    "\n",
    "# Extract 'County' and 'State' information (as shown in the previous response)\n",
    "ghgrp_production['County'] = ghgrp_production['sub_basin_county'].str.extract(r'([A-Z\\s]+),\\s[A-Z]+\\s\\(\\d+\\)')\n",
    "ghgrp_production['State_abbr'] = ghgrp_production['sub_basin_county'].str.extract(r'([A-Z]+)\\s\\(\\d+\\)')\n",
    "\n",
    "# Add a new column 'State' with the full state names\n",
    "ghgrp_production['State'] = ghgrp_production['State_abbr'].map(state_abbreviations_to_names).str.upper()\n",
    "\n",
    "# Add production data to GHGRP\n",
    "\n",
    "GHGRPfolder = os.sep.join([root_path, 'ghgrp'])\n",
    "wells_grouped_by_year_folder = root_path + '/wells_info_prod_per_basin'  # Latest data folder\n",
    "\n",
    "# Collect all files from the EF_W_ONSHORE_WELLS folder\n",
    "wells_folder = os.path.join(GHGRPfolder, 'EF_W_ONSHORE_WELLS')\n",
    "all_files = [os.path.join(wells_folder, file) for file in os.listdir(wells_folder) if file.endswith('.xlsb')]\n",
    "\n",
    "# Load and concatenate all EF_W_ONSHORE_WELLS data with progress tracking\n",
    "wells_data_list = []\n",
    "print(\"Loading EF_W_ONSHORE_WELLS files...\")\n",
    "with tqdm(total=len(all_files)) as pbar:\n",
    "    for file in all_files:\n",
    "        wells_data = pd.read_excel(file, usecols=['FACILITY_ID', 'WELL_ID_NUMBER', 'SUB_BASIN'], engine='pyxlsb')\n",
    "\n",
    "        # Extract the year from the filename (assuming year is in the filename)\n",
    "        year = ''.join(filter(str.isdigit, os.path.basename(file)))\n",
    "        wells_data['Year'] = year\n",
    "\n",
    "        wells_data_list.append(wells_data)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "wells_data = pd.concat(wells_data_list, ignore_index=True)\n",
    "\n",
    "wells_data = wells_data.dropna(subset=['WELL_ID_NUMBER'])\n",
    "\n",
    "wells_data['WELL_ID_NUMBER'] = wells_data['WELL_ID_NUMBER'].astype(str)\n",
    "\n",
    "# Convert scientific notation to full numeric format\n",
    "scientific_notation_wells = wells_data[\n",
    "    wells_data['WELL_ID_NUMBER'].str.contains(r'E\\+', case=False, na=False)]\n",
    "\n",
    "# Convert these values to floats, then back to full numbers (string format)\n",
    "wells_data.loc[scientific_notation_wells.index, 'WELL_ID_NUMBER'] = \\\n",
    "    wells_data.loc[scientific_notation_wells.index, 'WELL_ID_NUMBER'].apply(lambda x: '{:.0f}'.format(float(x)))\n",
    "\n",
    "# Remove dashes ('-') if present\n",
    "wells_data['WELL_ID_NUMBER'] = wells_data['WELL_ID_NUMBER'].str.replace('-', '')\n",
    "\n",
    "# Identify and remove rows where WELL_ID_NUMBER contains letters\n",
    "# wells_data = wells_data[~wells_data['WELL_ID_NUMBER'].str.contains(r'[A-Za-z]', regex=True, na=False)]\n",
    "wells_data['WELL_ID_NUMBER'].str.replace(r'\\D+', '', regex=True)\n",
    "\n",
    "# Convert numeric well IDs to int and back to string (removes leading zeros)\n",
    "wells_data['WELL_ID_NUMBER'] = wells_data['WELL_ID_NUMBER'].str.lstrip('0')  # Remove leading zeros\n",
    "\n",
    "# Remove well IDs shorter than 9 characters\n",
    "# wells_data = wells_data[wells_data['WELL_ID_NUMBER'].str.len() >= 9]\n",
    "\n",
    "# Rename WELL_ID_NUMBER to API14 for merging\n",
    "\n",
    "wells_data['WELL_ID_NUMBER'] = wells_data['WELL_ID_NUMBER'].apply(\n",
    "    lambda x: x.ljust(14, '0') if len(x) == 12 else\n",
    "    x.ljust(13, '0') if len(x) == 11 else\n",
    "    x.ljust(14, '0') if len(x) == 10 else\n",
    "    x.ljust(13, '0') if len(x) == 9 else x\n",
    ")\n",
    "\n",
    "# Load all Enverus well production data grouped by year with progress tracking\n",
    "print(\"Loading well production data...\")\n",
    "all_production_files = [\n",
    "    os.path.join(wells_grouped_by_year_folder, file)\n",
    "    for file in os.listdir(wells_grouped_by_year_folder)\n",
    "]\n",
    "\n",
    "production_data_list = []\n",
    "for file in tqdm(all_production_files, desc=\"Processing well production files\"):\n",
    "    production_data = pd.read_csv(file)\n",
    "    # Filter to keep only years 2015 and later\n",
    "    production_data = production_data[production_data['Year'] >= 2015]\n",
    "    production_data_list.append(production_data)\n",
    "production_data = pd.concat(production_data_list, ignore_index=True)\n",
    "\n",
    "production_data['API14'] = production_data['API14'].astype(int)\n",
    "production_data['API14'] = production_data['API14'].astype(str)\n",
    "production_data['API14'] = production_data['API14'].str.lstrip('0')  # Remove leading zeros\n",
    "\n",
    "wells_data['Year'] = wells_data['Year'].astype(int)\n",
    "\n",
    "# Merge the production data with wells data for the same year\n",
    "api_facility_correspondance_df = production_data.merge(\n",
    "    wells_data, \n",
    "    left_on=['API14', 'Year'], \n",
    "    right_on=['WELL_ID_NUMBER', 'Year'],\n",
    "    how='inner'  # Ensure we only keep matches\n",
    ")\n",
    "\n",
    "wells_ghgrp_prod = api_facility_correspondance_df.merge(ghgrp_production, left_on=['FACILITY_ID', 'Year','SUB_BASIN'], right_on=['facility_id', 'reporting_year','sub_basin_identifier'])\n",
    "\n",
    "wells_ghgrp_prod = wells_ghgrp_prod.drop(columns={'WELL_ID_NUMBER', 'basin_associated_with_facility', 'facility_name',\n",
    "       'fractionate_ngl', 'gas_oil_well_ratio', 'gas_prod_cal_year_for_sales',\n",
    "       'gas_prod_cal_year_from_wells', 'facility_id', 'industry_segment',\n",
    "       'miles_of_trans_pipe', 'oil_prod_cal_year_for_sales',\n",
    "       'producing_wells_acquired', 'producing_wells_divested',\n",
    "       'quantity_gas_added', 'quantity_gas_consumed', 'quantity_gas_delivered',\n",
    "       'quantity_gas_received', 'quantity_gas_stolen',\n",
    "       'quantity_gas_transferred', 'quantity_gas_withdrawn',\n",
    "       'quantity_imported', 'quantity_of_gas_injected',\n",
    "       'quant_gas_transported_gb', 'quantity_of_gas_withdrawn',\n",
    "       'quant_hc_liq_recd', 'quant_hc_liq_trans', 'reporting_year',\n",
    "       'sub_basin_county', 'sub_basin_formation_type', 'sub_basin_identifier',\n",
    "       'us_state', 'well_producing_end_of_year',\n",
    "       'well_removed_from_production', 'wells_completed', 'County',\n",
    "       'State_abbr', 'State'})\n",
    "\n",
    "wells_ghgrp_prod = wells_ghgrp_prod.rename(columns={'ch4_average_mole_fraction':'C1', 'co2_average_mole_fraction':'CO2'})\n",
    "wells_ghgrp_prod = wells_ghgrp_prod.drop_duplicates()\n",
    "\n",
    "wells_ghgrp_prod['C1'] *= 100\n",
    "\n",
    "wells_ghgrp_prod['CO2'] *= 100\n",
    "\n",
    "output_filename = 'ghgrp_production_processed.csv'\n",
    "output_path = os.path.join(GHGRPfolder, output_filename)\n",
    "wells_ghgrp_prod.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cbed7-d4ef-430f-86a5-c96f0ac0a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghgrp = pd.read_csv(ghgrp_path)\n",
    "ghgrp_indsegm_not_na = ghgrp[ghgrp.industry_segment.notna()]\n",
    "ghgrp_processing = ghgrp_indsegm_not_na[ghgrp_indsegm_not_na.industry_segment.str.contains('Onshore natural gas processing')]\n",
    "# remove 0 values\n",
    "ghgrp_processing = ghgrp_processing[ghgrp_processing.ch4_average_mole_fraction > 0].reset_index(drop=True)\n",
    "\n",
    "ghgrp_processing = ghgrp_processing.merge(facilities_lat_lon, left_on=['facility_id'], right_on=['facility_id'])\n",
    "\n",
    "geometry = [Point(lon, lat) for lat, lon in zip(ghgrp_processing.latitude, ghgrp_processing.longitude)]\n",
    "\n",
    "ghgrp_processing_gdf = gpd.GeoDataFrame(ghgrp_processing, geometry=geometry, crs='EPSG:4326')\n",
    "ghgrp_processing_gdf = gpd.sjoin(ghgrp_processing_gdf.to_crs(26914), us_counties.to_crs(26914), op='within')\n",
    "\n",
    "ghgrp_processing_gdf[['County', 'State']] = ghgrp_processing_gdf['NAME'].str.extract(r'(.+)\\s+\\((.+)\\)')\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.drop(columns={'index_right'})\n",
    "ghgrp_processing_gdf = gpd.sjoin(ghgrp_processing_gdf.to_crs(26914), basins_gdf.to_crs(26914), op='within')\n",
    "\n",
    "capacities = pd.read_csv(root_path + '/ghgrp/processing_capacities.csv',  usecols=['State', \n",
    "                                                                                   'County',\n",
    "                                                                                   'Cap_MMcfd',\n",
    "                                                                                  'Plant_Flow',\n",
    "                                                                                  'x',\n",
    "                                                                                  'y'])\n",
    "geometry = [Point(lon, lat) for lat, lon in zip(capacities.y, capacities.x)]\n",
    "capacities_gdf = gpd.GeoDataFrame(capacities, geometry=geometry, crs='EPSG:3857')\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.validation import explain_validity\n",
    "\n",
    "def nearest(row, other_gdf):\n",
    "    \"\"\"\n",
    "    Find the nearest point in `other_gdf` for the point in the row of `gdf1`.\n",
    "    \n",
    "    row: A row from gdf1\n",
    "    other_gdf: The second GeoDataFrame (gdf2) to find the nearest point.\n",
    "    \n",
    "    Returns:\n",
    "        The geometry and index of the nearest point from `other_gdf`.\n",
    "    \"\"\"\n",
    "    # Validate geometry before processing\n",
    "    if not row.geometry.is_valid:\n",
    "        print(row)\n",
    "        print(f\"Invalid geometry found: {explain_validity(row.geometry)}\")\n",
    "        return None, None\n",
    "\n",
    "    # Use shapely's nearest_points to find the closest point\n",
    "    try:\n",
    "        nearest_geom = nearest_points(row.geometry, other_gdf.unary_union)[1]\n",
    "        # Find the corresponding index of the nearest point in gdf2\n",
    "        nearest_geom_idx = other_gdf.distance(nearest_geom).idxmin()\n",
    "        return nearest_geom, nearest_geom_idx\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding nearest point for geometry: {row.geometry} - {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def get_nearest_distances(gdf1, gdf2):\n",
    "    \"\"\"\n",
    "    Find the nearest point in gdf2 for each point in gdf1, compute the distance,\n",
    "    and return the corresponding row content from gdf2.\n",
    "    \n",
    "    gdf1: GeoDataFrame with point geometries (source points)\n",
    "    gdf2: GeoDataFrame with point geometries (target points)\n",
    "    \n",
    "    Returns:\n",
    "        A GeoDataFrame with distances and nearest point information from gdf2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure both GeoDataFrames are in the same CRS\n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        gdf1 = gdf1.to_crs(gdf2.crs)\n",
    "\n",
    "    # Filter out invalid or empty geometries in gdf2\n",
    "    gdf2 = gdf2[gdf2.is_valid & ~gdf2.is_empty]\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    nearest_geometries = []\n",
    "    nearest_indices = []\n",
    "    distances = []\n",
    "    \n",
    "    # Find the nearest point in gdf2 for each point in gdf1\n",
    "    for idx, row in gdf1.iterrows():\n",
    "        nearest_geom, nearest_geom_idx = nearest(row, gdf2)\n",
    "        nearest_geometries.append(nearest_geom)\n",
    "        nearest_indices.append(nearest_geom_idx)\n",
    "        distances.append(row.geometry.distance(nearest_geom) if nearest_geom else None)\n",
    "\n",
    "    # Add the nearest distances and geometries to gdf1\n",
    "    gdf1['distance_to_nearest'] = distances\n",
    "    gdf1['nearest_geom'] = nearest_geometries\n",
    "\n",
    "    # Merge the nearest row from gdf2 based on the nearest_indices\n",
    "    nearest_rows = gdf2.loc[nearest_indices].reset_index(drop=True)\n",
    "    gdf1 = gdf1.reset_index(drop=True)\n",
    "\n",
    "    # Merge gdf1 with nearest_rows from gdf2\n",
    "    gdf1 = gdf1.join(nearest_rows, rsuffix='_gdf2')\n",
    "\n",
    "    return gdf1\n",
    "\n",
    "# Example usage\n",
    "ghgrp_processing_gdf = get_nearest_distances(capacities_gdf, ghgrp_processing_gdf)\n",
    "\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.drop(columns={'NAME','index_right','BASIN_CODE'})\n",
    "\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.rename(columns={'reporting_year':'Year'})\n",
    "\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.drop_duplicates()\n",
    "\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.reset_index(drop=True)\n",
    "\n",
    "ghgrp_processing_gdf = ghgrp_processing_gdf.rename(columns={'Monthly Gas':'Gas'})\n",
    "\n",
    "ghgrp_processing_gdf['Count'] = [1 for i in range(len(ghgrp_processing_gdf))]\n",
    "\n",
    "ghgrp_processing_gdf.to_csv(root_path + '/ghgrp_processing_new_weighted_variable_gdf.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
