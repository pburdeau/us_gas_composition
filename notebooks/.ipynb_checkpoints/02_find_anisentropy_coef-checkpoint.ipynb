{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275e447-6562-4fc4-b021-1424643738de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285a896-d2e2-4064-b12e-6592b501aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload root_path\n",
    "\n",
    "root_path = '/scratch/users/pburdeau/data/gas_composition'\n",
    "\n",
    "usgs_data = pd.read_csv(os.path.join(root_path, 'usgs/usgs_processed_with_nanapis.csv'))  # path to usgs data\n",
    "\n",
    "basins_to_keep = ['Appalachian Basin',\n",
    "              'Appalachian Basin (Eastern Overthrust Area)',\n",
    "              'Permian Basin',\n",
    "              'Arkla Basin',\n",
    "              'Anadarko Basin',\n",
    "              'San Joaquin Basin',\n",
    "              'Denver Basin',\n",
    "              'Uinta Basin',\n",
    "              'Green River Basin',\n",
    "              'Arkoma Basin',\n",
    "            'Gulf Coast Basin (LA, TX)',\n",
    "              'Williston Basin',\n",
    "               'East Texas Basin']\n",
    "usgs_gdf_main_basins = usgs_data[usgs_data.BASIN_NAME.isin(basins_to_keep)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef3880-0b01-4b3b-b77a-28030ddce395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmse(empVgm, stAni):\n",
    "    # Filter data where spacelag is 0 and fit a linear model\n",
    "    lm_data = empVgm[empVgm['spacelag'] <= 5000]\n",
    "    if len(lm_data) > 10:\n",
    "        lm_formula = 'gamma ~ timelag'\n",
    "        lm_model = ols(lm_formula, data=lm_data).fit()\n",
    "\n",
    "        filtered_data = empVgm[empVgm['timelag'] <= 365]\n",
    "        dist_adjusted = filtered_data['spacelag'] / stAni\n",
    "        predicted_gamma = lm_model.predict(exog=dict(timelag=dist_adjusted))\n",
    "        rmse = np.sqrt(np.mean((predicted_gamma - filtered_data['gamma'])**2))\n",
    "\n",
    "        return rmse\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def compute_diffs_time_space_basin(basin, comp, alpha):\n",
    "    usgs_basin_gdf = usgs_gdf_main_basins[usgs_gdf_main_basins.BASIN_NAME == basin]\n",
    "    usgs_basin_gdf = usgs_basin_gdf[~pd.isna(usgs_basin_gdf[comp])]\n",
    "    \n",
    "    # Convert geometry to a list of tuples\n",
    "    points = list(zip(usgs_basin_gdf['X'], usgs_basin_gdf['Y']))\n",
    "    values = usgs_basin_gdf[comp].astype(float).to_numpy().reshape(-1,1)\n",
    "    \n",
    "    if len(values) > 10:\n",
    "        nst_trans = QuantileTransformer(n_quantiles=500, output_distribution='normal').fit(values)\n",
    "        values_trans = nst_trans.transform(values)\n",
    "\n",
    "        values = values_trans.reshape(-1)\n",
    "        del values_trans\n",
    "\n",
    "        epochs = usgs_basin_gdf['T'].to_numpy()\n",
    "        years = usgs_basin_gdf['Year'].to_numpy()\n",
    "        num_points = len(points)\n",
    "\n",
    "        distances_space = pdist(points)\n",
    "\n",
    "        distances_space = squareform(distances_space)\n",
    "\n",
    "        # Calculate temporal distances\n",
    "        alpha_squared = alpha ** 2\n",
    "        distances_time = np.abs(epochs[:, None] - epochs[None, :])\n",
    "\n",
    "        # Calculate spatiotemporal distances\n",
    "        distances = np.sqrt(distances_space**2 + (alpha_squared * distances_time**2))\n",
    "\n",
    "        # Calculate differences in values\n",
    "        diffs = np.abs(values[:, None] - values[None, :]) ** 2\n",
    "\n",
    "        # Flatten matrices\n",
    "        mask_upper_triangle = np.triu(np.ones(num_points, dtype=bool), k=1)\n",
    "        diffs = diffs[mask_upper_triangle]\n",
    "        distances = distances[mask_upper_triangle]\n",
    "        distances_time = distances_time[mask_upper_triangle]\n",
    "        distances_space = distances_space[mask_upper_triangle]\n",
    "\n",
    "        return diffs, distances, distances_time, distances_space\n",
    "    else:\n",
    "        return [0], [0], [0], [0]\n",
    "\n",
    "def compute_variances_and_bins(diffs, distances_time, distances_space, alphas, n_bins=6):\n",
    "    \n",
    "    variances_by_alpha = {}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        if alpha < 1:\n",
    "            time_bins = np.linspace(np.min(distances_time) * alpha/(4.), np.max(distances_time) * alpha/(4.), n_bins)\n",
    "            space_bins = np.linspace(np.min(distances_space)/(4.), np.max(distances_space)/(4.), n_bins) # 1m is 5 epochs\n",
    "        else:\n",
    "            time_bins = np.linspace(np.min(distances_time)/(4.), np.max(distances_time) /(4.), n_bins)\n",
    "            scaled_space_bins = time_bins\n",
    "            space_bins = np.linspace(np.min(distances_space)/(4. * alpha), np.max(distances_space)/(4. * alpha), n_bins) # 1m is 5 epochs\n",
    "        adjusted_time_distances = distances_time * alpha\n",
    "        adjusted_space_distances = distances_space * alpha\n",
    "\n",
    "        time_indices = np.digitize(distances_time, time_bins) - 1\n",
    "        space_indices = np.digitize(np.array(distances_space) / alpha, scaled_space_bins) - 1\n",
    "\n",
    "        variances = np.zeros((len(time_bins)-1, len(space_bins)-1, 3))  # 3 for original, time adjusted, space adjusted\n",
    "        \n",
    "        for i in range(len(time_bins)-1):\n",
    "            for j in range(len(space_bins)-1):\n",
    "                mask_original = (time_indices == i) & (space_indices == j)\n",
    "                \n",
    "                if np.any(mask_original):\n",
    "                    variances[i, j, 0] = np.mean(diffs[mask_original])\n",
    "\n",
    "        variances_by_alpha[alpha] = variances\n",
    "\n",
    "    return time_bins, scaled_space_bins, variances_by_alpha\n",
    "\n",
    "def compute_variances_and_bins_for_est(diffs, distances_time, distances_space, alphas, n_bins=6):\n",
    "    \n",
    "    variances_by_alpha = {}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        if alpha < 1:\n",
    "            time_bins = np.linspace(np.min(distances_time) * alpha/(4.), np.max(distances_time) * alpha/(4.), n_bins)\n",
    "            space_bins = np.linspace(np.min(distances_space)/(4.), np.max(distances_space)/(4.), n_bins) # 1m is 5 epochs\n",
    "        else:\n",
    "            time_bins = np.linspace(np.min(distances_time)/(4.), np.max(distances_time) /(4.), n_bins)\n",
    "            space_bins = np.linspace(np.min(distances_space)/(4. * alpha), np.max(distances_space)/(4. * alpha), n_bins) # 1m is 5 epochs\n",
    "        adjusted_time_distances = distances_time * alpha\n",
    "        adjusted_space_distances = distances_space * alpha\n",
    "\n",
    "        time_indices = np.digitize(distances_time, time_bins) - 1\n",
    "        space_indices = np.digitize(distances_space, space_bins) - 1\n",
    "\n",
    "        variances = np.zeros((len(time_bins)-1, len(space_bins)-1, 3))  # 3 for original, time adjusted, space adjusted\n",
    "        \n",
    "        for i in range(len(time_bins)-1):\n",
    "            for j in range(len(space_bins)-1):\n",
    "                mask_original = (time_indices == i) & (space_indices == j)\n",
    "                \n",
    "                if np.any(mask_original):\n",
    "                    variances[i, j, 0] = np.mean(diffs[mask_original])\n",
    "\n",
    "        variances_by_alpha[alpha] = variances\n",
    "\n",
    "    return time_bins, space_bins, variances_by_alpha\n",
    "\n",
    "def create_dataframe_from_variances(variances_by_alpha, space_bins, time_bins, alphas):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from semivariance calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    - variances_by_alpha: dict, semivariances for each alpha indexed by space and time bins\n",
    "    - space_bins: array-like, edges of space bins\n",
    "    - time_bins: array-like, edges of time bins\n",
    "    - alphas: list, alpha values used in the semivariance calculation\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns ['alpha', 'space_bin', 'time_bin', 'gamma']\n",
    "    \"\"\"\n",
    "     # Pre-calculate bin centers for more accurate representation\n",
    "    space_bin_centers = (space_bins[:-1] + space_bins[1:]) / 2\n",
    "    time_bin_centers = (time_bins[:-1] + time_bins[1:]) / 2\n",
    "    \n",
    "    # Prepare data collection\n",
    "    records = []\n",
    "\n",
    "    for i, time_center in enumerate(time_bin_centers):\n",
    "        for j, space_center in enumerate(space_bin_centers):\n",
    "            gamma_value = variances_by_alpha[i, j]\n",
    "            # Append each combination of alpha, space, time, and gamma to records\n",
    "            records.append({\n",
    "                'spacelag': space_center,\n",
    "                'timelag': time_center,\n",
    "                'gamma': gamma_value\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from records\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed45ad3-49dc-4109-be3d-ff794c6b9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estiStAni_lin_space(empVgm, interval, figures_path, colormap):\n",
    "    # Ensure the output directory exists\n",
    "    import os\n",
    "    os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "    # Filter data where spacelag is 0 and fit a linear model\n",
    "    lm_data = empVgm[empVgm['spacelag'] <= 5000]\n",
    "    if len(lm_data) > 10:\n",
    "        lm_formula = 'gamma ~ timelag'\n",
    "        lm_model = ols(lm_formula, data=lm_data).fit()\n",
    "\n",
    "        def optFun(stAni, empVgm, plot=False, label=None, colormap=None):\n",
    "            filtered_data = empVgm[empVgm['timelag'] <= 365]\n",
    "            dist_adjusted = filtered_data['spacelag'] / stAni\n",
    "            predicted_gamma = lm_model.predict(exog=dict(timelag=dist_adjusted))\n",
    "            rmse = np.sqrt(np.mean((predicted_gamma - filtered_data['gamma'])**2))\n",
    "\n",
    "            if plot:\n",
    "                stAni_scalar = stAni[0] if isinstance(stAni, (np.ndarray, list)) else stAni  # Extract scalar if needed\n",
    "                fig, ax1 = plt.subplots(figsize=(11, 8))\n",
    "                plt.rcParams.update({'font.size': 26})\n",
    "\n",
    "                # Create fixed colors from the colormap\n",
    "                cmap = colormap or sns.cubehelix_palette(start=2.1, hue=1.7, light=0.9, dark=0.2, rot=0.4, as_cmap=True, reverse=True)\n",
    "                colors = [cmap(0.2), cmap(0.5)]  # Two fixed tones from the colormap\n",
    "                # Plot empirical variogram on the primary x-axis (spatial distances in meters)\n",
    "                scatter1 = ax1.scatter(\n",
    "                    filtered_data['spacelag'] / 1000,\n",
    "                    filtered_data['gamma'],\n",
    "                    label=r\"Empirical spatial variogram ($\\gamma_s$)\",\n",
    "                    color=colors[0],\n",
    "                    s=50\n",
    "                )\n",
    "                ax1.set_xlabel(r\"Spatial distance ($D_s$) [km]\")\n",
    "                ax1.set_ylabel(\"Semivariance [mol %$^2$]\")\n",
    "\n",
    "                # Plot predicted variogram aligned to temporal distances\n",
    "                scatter2 = ax1.scatter(\n",
    "                    filtered_data['spacelag'] / 1000,\n",
    "                    predicted_gamma,\n",
    "                    label=r\"$\\beta_0 + \\beta_1 \\cdot \\frac{D_s}{\\alpha}$\",\n",
    "                    color=colors[1],\n",
    "                    alpha=0.6,\n",
    "                    s=50\n",
    "                )\n",
    "                \n",
    "                ax1.set_title(f\"$\\\\alpha = {stAni_scalar:.1f}$, RMSE: {rmse:.1f} mol %$^2$\\n\")\n",
    "                legend = ax1.legend(loc=\"upper left\", fontsize=26)\n",
    "                legend.get_frame().set_alpha(0.5)  # Set transparency to 50%\n",
    "\n",
    "                # Save the plot\n",
    "                if label:\n",
    "                    filename = f\"plot_stAni_{label}.eps\"\n",
    "                else:\n",
    "                    filename = f\"plot_stAni_{stAni_scalar:.2f}.eps\"\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(figures_path, filename), format='eps', dpi=300)\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "\n",
    "            return rmse\n",
    "\n",
    "\n",
    "\n",
    "        # Wrapper function for optimization\n",
    "        iteration_count = [0]\n",
    "\n",
    "        def optFunWrapper(stAni, *args):\n",
    "            rmse = optFun(stAni, *args)\n",
    "            if iteration_count[0] % 10 == 0:\n",
    "                optFun(stAni, *args, plot=True)\n",
    "            iteration_count[0] += 1\n",
    "            return rmse\n",
    "\n",
    "        # Run optimization\n",
    "        result = minimize(\n",
    "            fun=optFunWrapper,\n",
    "            x0=[np.mean(interval)],\n",
    "            args=(empVgm,),\n",
    "            method='Nelder-Mead',\n",
    "            options={'xatol': 1e-1, 'fatol': 1e-10}\n",
    "        )\n",
    "\n",
    "        # Plot optimal result\n",
    "        optFun(result.x[0], empVgm, plot=True, label=\"optimal\")\n",
    "\n",
    "        # Plot for stAni = 1\n",
    "        optFun(1, empVgm, plot=True, label=\"1\")\n",
    "\n",
    "        # Plot for stAni = 40\n",
    "        optFun(40, empVgm, plot=True, label=\"40\")\n",
    "\n",
    "        return result.x[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# To integrate it into your complete function with specific parameters, pass this to the `estiStAni_lin_space` function.\n",
    "# Please let me know if you need me to connect it explicitly.\n",
    "\n",
    "\n",
    "alphas = [1]\n",
    "\n",
    "\n",
    "def plot_3d(alphas, diffs, distances_time, distances_space, nbins=6):\n",
    "    alpha = alphas[0]\n",
    "    time_bins, space_bins, variances_by_alpha = compute_variances_and_bins(diffs, distances_time, distances_space, alphas)\n",
    "    space_bins_sorted = np.sort(space_bins)\n",
    "    time_bins_sorted = np.sort(time_bins)\n",
    "\n",
    "    space_centers = (space_bins_sorted[:-1] + space_bins_sorted[1:]) / 2\n",
    "    time_centers = (time_bins_sorted[:-1] + time_bins_sorted[1:]) / 2\n",
    "\n",
    "    X, Y = np.meshgrid(space_centers, time_centers)\n",
    "    Z = variances_by_alpha[alpha][..., 0] \n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "    ax.set_ylabel('Time')\n",
    "    ax.set_xlabel('Space')\n",
    "    ax.set_zlabel('Variance')\n",
    "    ax.invert_xaxis()\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    \n",
    "    ax.view_init(elev=20, azim=-45)  # You can adjust these angles to get the desired viewpoint\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    contour = ax.contourf(X, Y, Z, 50, cmap='viridis', extend3d=True)\n",
    "    ax.set_ylabel('Time')\n",
    "    ax.set_xlabel('Space')\n",
    "    ax.set_zlabel('Variance')\n",
    "    ax.invert_xaxis()\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    \n",
    "    ax.view_init(elev=20, azim=-45)  # You can adjust these angles to get the desired viewpoint\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a94c0-993e-42b2-85d2-3709328a55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(alphas, diffs, distances_time, distances_space, nbins=6, cubehelix_custom=sns.cubehelix_palette(start=0.2, hue=1.4, light=0.8, dark=0.3, as_cmap=True, reverse=True)):\n",
    "    vmin, vmax = 0, 3  # Fixed colorbar range\n",
    "    fontsize = 26\n",
    "    alpha = alphas[0]\n",
    "    time_bins, space_bins, variances_by_alpha = compute_variances_and_bins(diffs, distances_time, distances_space, alphas, nbins)\n",
    "    space_bins_sorted = np.sort(space_bins)\n",
    "    time_bins_sorted = np.sort(time_bins)\n",
    "\n",
    "    space_centers = (space_bins_sorted[:-1] + space_bins_sorted[1:]) / 2\n",
    "    time_centers = (time_bins_sorted[:-1] + time_bins_sorted[1:]) / 2\n",
    "\n",
    "    X, Y = np.meshgrid(space_centers, time_centers)\n",
    "    Z = variances_by_alpha[alpha][..., 0]\n",
    "\n",
    "    # Set common limits\n",
    "    common_limit = max(space_centers[-1], time_centers[-1])\n",
    "    common_ticks = np.linspace(0, common_limit, 6)  # 6 evenly spaced ticks\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    heatmap = ax.imshow(Z, cmap=cubehelix_custom, aspect='auto', origin='lower',\n",
    "                        extent=[0, common_limit, 0, common_limit])\n",
    "    ax.set_ylabel('Time difference [days]', fontsize=fontsize)\n",
    "    ax.set_xlabel(f'Scaled space difference [m / {alpha}]', fontsize=fontsize)\n",
    "    ax.set_title(rf'$\\alpha$ = {alpha}', fontsize=fontsize)\n",
    "    fig.colorbar(heatmap, ax=ax, label='Variance')\n",
    "\n",
    "    # Synchronize ticks\n",
    "    ax.set_xticks(common_ticks)\n",
    "    ax.set_yticks(common_ticks)\n",
    "\n",
    "    # Apply scientific notation formatter\n",
    "    def format_ticks(value, _):\n",
    "        if value == 0:\n",
    "            return \"0\"\n",
    "        power = int(np.floor(np.log10(abs(value))))\n",
    "        factor = int(value / (10**power))\n",
    "        return f\"{factor}e{power}\"\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mticker.FuncFormatter(format_ticks))\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(format_ticks))\n",
    "    ax.tick_params(axis='x', labelsize=fontsize)\n",
    "    ax.tick_params(axis='y', labelsize=fontsize)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(1000, 7000)\n",
    "    plt.savefig(f'figures_out/heatmap_alpha_{alpha:.1f}.eps', format='eps', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot contour map\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    # Filled contours\n",
    "    contour = ax.contourf(X, Y, Z, levels=np.linspace(vmin, vmax, 25), cmap=cubehelix_custom, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Add contour lines on top\n",
    "    lines = ax.contour(X, Y, Z, levels=np.linspace(vmin, vmax, 5), colors='black', linewidths=0.5)\n",
    "    ax.clabel(lines, inline=True, fontsize=fontsize - 6, fmt=\"%.1f\")  # Optional: label lines\n",
    "\n",
    "    ax.set_ylabel('Temporal lag [days]', fontsize=fontsize)\n",
    "    ax.set_xlabel(rf'Rescaled spatial lag [days]', fontsize=fontsize)\n",
    "    ax.set_title(rf'$\\alpha$ = {alpha:.1f}', fontsize=fontsize)\n",
    "\n",
    "\n",
    "\n",
    "    # Synchronize ticks for contour map\n",
    "    ax.set_xticks(common_ticks)\n",
    "    ax.set_yticks(common_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mticker.FuncFormatter(format_ticks))\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(format_ticks))\n",
    "    ax.tick_params(axis='x', labelsize=fontsize)\n",
    "    ax.tick_params(axis='y', labelsize=fontsize)\n",
    "    ax.set_xlim(1000, 7000)\n",
    "    ax.set_ylim(1000, 7000)\n",
    "    \n",
    "\n",
    "\n",
    "    cbar = fig.colorbar(contour, ax=ax)\n",
    "    cbar.set_label(r'Variance [mol %$^2$]', fontsize=fontsize)\n",
    "    cbar.ax.tick_params(labelsize=fontsize - 2)\n",
    "#     cbar.set_ticks(np.linspace(vmin, vmax, 5))\n",
    "#     cbar.ax.set_yticklabels([f'{tick:.1f}' for tick in np.linspace(vmin, vmax, 5)])\n",
    "\n",
    "\n",
    "    num_ticks = 5  # Set the desired number of ticks\n",
    "    tick_values = np.linspace(vmin, vmax, num_ticks)  # Generate evenly spaced ticks\n",
    "    cbar.set_ticks(tick_values)  # Set the custom ticks\n",
    "    cbar.ax.set_yticklabels([f'{tick:.1f}' for tick in tick_values])\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    plt.savefig(f'figures_out/contourmap_alpha_{alpha:.1f}.eps', format='eps', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8d235-68f5-4cdd-881e-20af859883fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'C1'\n",
    "alpha = 1\n",
    "\n",
    "# create array of all diffs within basins\n",
    "\n",
    "all_diffs = []\n",
    "all_distances_time = []\n",
    "all_distances_space = []\n",
    "all_distances = []\n",
    "for basin in basins_to_keep:\n",
    "    diffs, distances, distances_time, distances_space = compute_diffs_time_space_basin(basin, comp, alpha)\n",
    "    all_diffs = np.concatenate([all_diffs, diffs])\n",
    "    all_distances_time = np.concatenate([all_distances_time, distances_time])\n",
    "    all_distances_space = np.concatenate([all_distances_space, distances_space])\n",
    "    all_distances = np.concatenate([all_distances, distances])\n",
    "\n",
    "\n",
    "res = []\n",
    "    \n",
    "alphas = [1]\n",
    "time_bins, space_bins, variances_by_alpha = compute_variances_and_bins_for_est(all_diffs, all_distances_time, all_distances_space, alphas, 50)\n",
    "\n",
    "df_variances = create_dataframe_from_variances(variances_by_alpha[alphas[0]][..., 0], space_bins, time_bins, alphas)\n",
    "\n",
    "plt.scatter(df_variances[df_variances.spacelag <= 5000].timelag, df_variances[df_variances.spacelag <= 5000].gamma)\n",
    "plt.xlim(0,10000)\n",
    "\n",
    "plt.scatter(df_variances[df_variances.timelag <= 200].spacelag, df_variances[df_variances.timelag <= 200].gamma)\n",
    "interval = [0.1,100]\n",
    "\n",
    "res.append(estiStAni_lin_space(df_variances, interval, figures_path='figures_out', colormap=sns.cubehelix_palette(start=2.1, hue=1.7, light=0.9, dark=0.2, rot=0.4, as_cmap=True, reverse=True)\n",
    "))\n",
    "stAnis = np.linspace(1, 50, 100)\n",
    "\n",
    "nbins = 6\n",
    "plot_2d([1], all_diffs, all_distances_time, all_distances_space, nbins, cubehelix_custom)\n",
    "\n",
    "nbins = 6\n",
    "plot_2d([6.34], all_diffs, all_distances_time, all_distances_space, nbins, cubehelix_custom)\n",
    "\n",
    "nbins = 6\n",
    "plot_2d([40], all_diffs, all_distances_time, all_distances_space, nbins, cubehelix_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411892cf-c117-470a-90e1-2da4e10d2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "estiStAni_lin_space(df_variances, interval, figures_path='figures_out', colormap=sns.cubehelix_palette(start=2.1, hue=1.7, light=0.9, dark=0.2, rot=0.4, as_cmap=True, reverse=True)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
