{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21100,
     "status": "ok",
     "timestamp": 1737744995350,
     "user": {
      "displayName": "Philippine Burdeau",
      "userId": "04256222199494352032"
     },
     "user_tz": 480
    },
    "id": "2TyACN8V8khB",
    "outputId": "ad273491-4c49-4c1a-e6ad-7a417e9cc735"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to access the required folders\n",
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mount the Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "base_folder = '/content/drive/Shared drives/Enverus US'\n",
    "\n",
    "output_folder = os.path.join(base_folder, 'Processed Data')\n",
    "raw_wells_production_folder = os.path.join(base_folder, 'Well Monthly Production - Anadarko')\n",
    "raw_wells_headers_folder = os.path.join(base_folder, 'Well Headers - Anadarko')\n",
    "wells_production_folder = os.path.join(output_folder, 'Well Monthly Production with useful columns - Anadarko')\n",
    "wells_headers_folder = os.path.join(output_folder, 'Well Headers with useful columns - Anadarko')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Base folder: {base_folder}\")\n",
    "print(f\"Wells production folder: {wells_production_folder}\")\n",
    "print(f\"Wells headers folder: {wells_headers_folder}\")\n",
    "print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "# Step 1: Process raw production files\n",
    "\n",
    "# Create a folder for processed files\n",
    "os.makedirs(wells_production_folder, exist_ok=True)\n",
    "\n",
    "# Filter files that match the specific names\n",
    "production_csv_files = [f for f in os.listdir(raw_wells_production_folder)]\n",
    "\n",
    "# Process each file individually\n",
    "for file in tqdm(production_csv_files, desc=\"Processing CSV files\"):\n",
    "    # Read only the necessary columns from the original file\n",
    "    file_path = os.path.join(raw_wells_production_folder, file)\n",
    "    print(f'Processing {file_path}')\n",
    "    processed_df = pd.read_csv(\n",
    "        file_path,\n",
    "        usecols=['API/UWI', 'Monthly Oil', 'Monthly Gas', 'Monthly Production Date']\n",
    "    )\n",
    "    processed_df = processed_df[~pd.isna(processed_df['API/UWI'])]\n",
    "\n",
    "    processed_df['API/UWI'] = processed_df['API/UWI'].astype(str)\n",
    "    # remove API = 0 because many occurrences\n",
    "    processed_df = processed_df[processed_df['API/UWI'] > '0']\n",
    "\n",
    "    # Save the processed file to the new folder\n",
    "    output_path = os.path.join(wells_production_folder, file)\n",
    "    processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed files saved to: {wells_production_folder}\")\n",
    "\n",
    "# Step 2: Process raw well headers files\n",
    "\n",
    "# Create a folder for processed files\n",
    "os.makedirs(wells_headers_folder, exist_ok=True)\n",
    "\n",
    "headers_csv_files = [f for f in os.listdir(raw_wells_headers_folder) if f.lower().endswith('.csv')]\n",
    "\n",
    "# Process each file individually\n",
    "for file in tqdm(headers_csv_files, desc=\"Processing CSV files\"):\n",
    "    # Read only the necessary columns from the original file\n",
    "    file_path = os.path.join(raw_wells_headers_folder, file)\n",
    "    processed_df = pd.read_csv(\n",
    "        file_path,\n",
    "        usecols=['API14', 'Surface Hole Latitude (WGS84)', 'Surface Hole Longitude (WGS84)']\n",
    "    )\n",
    "\n",
    "    processed_df = processed_df[~pd.isna(processed_df['API14'])]\n",
    "    processed_df['API14'] = processed_df['API14'].astype(str)\n",
    "    # remove API = 0\n",
    "    processed_df = processed_df[processed_df['API14'] != '0']\n",
    "\n",
    "    # Save the processed file to the new folder\n",
    "    output_path = os.path.join(wells_headers_folder, file)\n",
    "    processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed files saved to: {wells_headers_folder}\")\n",
    "\n",
    "# Step 3: Merge well header files with basins info\n",
    "\n",
    "def process_well_headers(headers_folder, basin_gdf, output_folder):\n",
    "    output_file = os.path.join(output_folder, 'wellheaderswithbasins.csv')\n",
    "    print(output_file)\n",
    "    print(\"Processing well header files...\")\n",
    "    headers_files = [f for f in os.listdir(headers_folder) if f.lower().endswith('.csv')]\n",
    "\n",
    "    # Concatenate well header files\n",
    "    headers_df = pd.concat(\n",
    "        [pd.read_csv(os.path.join(headers_folder, file)) for file in tqdm(headers_files, desc=\"Reading headers\")],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    headers_gdf = gpd.GeoDataFrame(\n",
    "        headers_df,\n",
    "        geometry=[Point(xy) for xy in zip(headers_df['Surface Hole Longitude (WGS84)'], headers_df['Surface Hole Latitude (WGS84)'])],\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    # First convert to the same crs (!)\n",
    "    # Spatial join with basins\n",
    "    headers_gdf = gpd.sjoin(headers_gdf.to_crs(26914), basin_gdf.to_crs(26914), how='inner', predicate='within')\n",
    "\n",
    "    # Save as a Pandas DataFrame with BASIN_NAME column\n",
    "    headers_gdf[['API14', 'Surface Hole Latitude (WGS84)', 'Surface Hole Longitude (WGS84)', 'BASIN_NAME']].to_csv(output_file, index=False)\n",
    "    print(f\"Processed well headers saved to: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Step 4: Merge production files with well headers (that have basin infos) to create initial output files\n",
    "\n",
    "def process_production_files(production_folder, headers_file, output_folder):\n",
    "    print(\"Reading Well Headers with Basins file...\")\n",
    "    headers_df = pd.read_csv(headers_file)\n",
    "\n",
    "    print(\"Looking at list of files...\")\n",
    "\n",
    "    production_files = [f for f in os.listdir(production_folder) if f.lower().endswith('.csv')]\n",
    "    print(\"Processing production files...\")\n",
    "    for file in tqdm(production_files, desc=\"Processing production files\"):\n",
    "        production_df = pd.read_csv(os.path.join(production_folder, file))\n",
    "        production_df['API/UWI'] = production_df['API/UWI'].astype(str)\n",
    "        headers_df['API14'] = headers_df['API14'].astype(str)\n",
    "        print(f\"Read file {file}. Merging with headers...\")\n",
    "\n",
    "        # Merge with headers\n",
    "        merged_df = pd.merge(\n",
    "            production_df,\n",
    "            headers_df,\n",
    "            left_on='API/UWI',\n",
    "            right_on='API14',\n",
    "            how='left'\n",
    "        )\n",
    "        print(\"Saving...\")\n",
    "\n",
    "        # Save each merged file\n",
    "        output_file = os.path.join(output_folder, f\"processed_{file}\")\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Processed production file saved to: {output_file}\")\n",
    "\n",
    "# Step 5: Split each initial output file into subfiles by BASIN_NAME\n",
    "\n",
    "def split_by_basin(input_folder, output_folder):\n",
    "    input_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.csv')]\n",
    "    print('Looping through files...')\n",
    "    for file in tqdm(input_files, desc=\"Splitting by BASIN_NAME\"):\n",
    "        if 'KS' in file:\n",
    "          df = pd.read_csv(os.path.join(input_folder, file))\n",
    "          print(f'Uploaded {file}')\n",
    "          # Split by BASIN_NAME\n",
    "          for basin_name, group in df.groupby('BASIN_NAME'):\n",
    "              print(basin_name)\n",
    "              basin_filename = f\"{basin_name.replace(' ', '_')}_{file}\"\n",
    "              basin_output_path = os.path.join(output_folder, basin_filename)\n",
    "              group.to_csv(basin_output_path, index=False)\n",
    "              print(f\"Saved basin file: {basin_output_path}\")\n",
    "\n",
    "# Step 6: Concatenate subfiles by BASIN_NAME\n",
    "\n",
    "def concat_by_basin(input_folder, final_output_folder, basins_gdf, processed_files=None):\n",
    "    \"\"\"\n",
    "    Concatenate files by BASIN_NAME and track processed files.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Folder containing input CSV files.\n",
    "        final_output_folder (str): Folder to save final concatenated CSV files.\n",
    "        basins_gdf (GeoDataFrame): GeoDataFrame with BASIN_NAME column.\n",
    "        processed_files (list): List of already processed files (optional).\n",
    "    \"\"\"\n",
    "    print(\"Concatenating files by BASIN_NAME...\")\n",
    "\n",
    "    # Initialize the processed files list if not provided\n",
    "    if processed_files is None:\n",
    "        processed_files = []\n",
    "\n",
    "    # Loop through all unique basin names\n",
    "    for basin_name in tqdm(basins_gdf.BASIN_NAME.unique(), desc=\"Processing basins\"):\n",
    "        print(f'Looking at {basin_name}...')\n",
    "\n",
    "        # Create a formatted version of the basin name to match filenames\n",
    "        formatted_basin_name = basin_name.replace(' ', '_')\n",
    "\n",
    "        # Find all files containing the basin name in their filename\n",
    "        matching_files = [\n",
    "            f for f in os.listdir(input_folder)\n",
    "            if f.lower().endswith('.csv') and f.split('_processed')[0] == formatted_basin_name\n",
    "        ]\n",
    "\n",
    "        # Filter out files that are already processed\n",
    "        matching_files = [f for f in matching_files if f not in processed_files]\n",
    "\n",
    "        # Skip if no matching files found\n",
    "        if not matching_files:\n",
    "            print(f\"No unprocessed files found for basin: {basin_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{len(matching_files)} unprocessed files found for {basin_name}, concatenating them...\")\n",
    "\n",
    "        # Concatenate all matching files\n",
    "        dfs = []\n",
    "        for file in matching_files:\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            dfs.append(pd.read_csv(file_path))\n",
    "            # Add the file to the processed files list\n",
    "            processed_files.append(file)\n",
    "\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Replace formatted names with the original BASIN_NAME\n",
    "        final_df['BASIN_NAME'] = basin_name\n",
    "        print('Saving the file...')\n",
    "\n",
    "        # Save the final DataFrame with the original BASIN_NAME in the filename\n",
    "        final_file = os.path.join(final_output_folder, f\"{basin_name}_final.csv\")\n",
    "        final_df.to_csv(final_file, index=False)\n",
    "        print(f\"Final concatenated file saved for basin {basin_name}: {final_file}\")\n",
    "\n",
    "        # Clear memory for this basin\n",
    "        del dfs, final_df\n",
    "\n",
    "    print(\"All basins processed.\")\n",
    "    return processed_files\n",
    "\n",
    "\n",
    "# Paths\n",
    "\n",
    "headers_output_folder = os.path.join(output_folder, 'Well Headers with Basins')\n",
    "merged_output_folder = os.path.join(output_folder, 'Well Monthly Production with Headers and Basins')\n",
    "split_output_folder = os.path.join(output_folder, 'Well Monthly Production with Headers and Basins split by Basin')\n",
    "final_output_folder = os.path.join(output_folder, 'Well Monthly Production with Headers and Basins concatenated by Basin')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(headers_output_folder, exist_ok=True)\n",
    "os.makedirs(merged_output_folder, exist_ok=True)\n",
    "os.makedirs(split_output_folder, exist_ok=True)\n",
    "os.makedirs(final_output_folder, exist_ok=True)\n",
    "\n",
    "# Run the steps\n",
    "basin_gdf_path = os.path.join(base_folder, 'Basins_Shapefile')\n",
    "basin_gdf = gpd.read_file(basin_gdf_path)\n",
    "\n",
    "process_well_headers(wells_headers_folder, basin_gdf, headers_output_folder)\n",
    "\n",
    "headers_file = os.path.join(headers_output_folder, 'wellheaderswithbasins.csv')\n",
    "process_production_files(wells_production_folder, headers_file, merged_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372069,
     "status": "ok",
     "timestamp": 1737682569130,
     "user": {
      "displayName": "Philippine Burdeau",
      "userId": "04256222199494352032"
     },
     "user_tz": 480
    },
    "id": "nvrJoI1WypNF",
    "outputId": "b722fcc4-b51f-406e-82b9-95bac2aca9b2"
   },
   "outputs": [],
   "source": [
    "split_by_basin(merged_output_folder, split_output_folder)\n",
    "\n",
    "processed_files = []\n",
    "\n",
    "# Run the function once\n",
    "processed_files = concat_by_basin(split_output_folder, final_output_folder, basin_gdf, processed_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUL5i059v3l4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7jiK7n/K7/OLw/wPPyIFi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
